\section{Computation-Aware Greedy Methods}
\label{sec:gomp_method}


\subsection{Preliminaries}
Before introducing our greedy methods for forming cost-efficient anytime predictors, 
we first formally state our assumptions and define some terminology.

We assume that all feature dimensions and responses are normalized to 
have zero mean and unit variance, i.e., we assume each column of $X$ has zero mean and unit variance.
We also assume the feature group costs $c(g)$ dominates the costs of computing linear predictions 
using the features.

We define the regularized feature covariance matrix as 
$C := \frac{1}{n}X^TX + \lambda I_D$. 
Let $C_{st}$ be the sub-matrix that selects rows from $s$ and columns from $t$. Let $C_S$ be short for $C_{SS}$. 
Given a non-empty union of selected feature groups $S$, the maximum explained variance 
$F(S)$ is achieved with the regularized optimal 
coefficient 
\begin{align}
w(S) 
&= \frac{1}{n}(\frac{1}{n}X_S^TX_S + \lambda I)^{-1}(X_S^TY) \\
&= \frac{1}{n} C_S^{-1}X_S^TY.
\label{eq:gomp_w}
\end{align}
When we take gradient of $F(S)$ with respect to the coefficient 
of a feature group $g$, if $g \subseteq S$ then the gradient is
\begin{align}
\nabla_g F(S) = \frac{1}{n} X_g^T(Y-X_Sw(S)) - \lambda w(S)_g.
\label{eq:gomp_g}
\end{align}
If $g \cap S  =\emptyset$ then we can extend $w(S)$ to dimensions of $g$, setting $w(S)_g = 0$, and then take the gradient to have 
\mbox{$\nabla_g F(S) =\frac{1}{n} X_g^T(Y-X_Sw(S))$}. In both cases,
we have 
\begin{align}
\nabla_g F(S) = \frac{1}{n} X_g^TY - C_{gS}w(S).
\label{eq:gomp_g}
\end{align} 
We further shorten the notations by defining $b_g^{S} = \nabla _g F(S)$. 
If $S$ is empty, we assume that coefficient $w(\emptyset)$ has zero for all features so that $F(\emptyset) = 0$. 
When $S = [s_1, s_2,...,]$ is a sequence of feature groups, we define
$S_j$ to be the prefix sequence $[s_1, s_2,..., s_j]$. We 
overload notations of a sequence $S$ so that $S$ also represents 
the set of features contained in the union of $s_1, s_2, ...,$ in notations such as $F(S)$, $w(S)$, $C_S$ and $b_S^S$, 
where we need the selected features in $S$ for evaluation and the ordering does not affect the computation.

\subsection{Anytime Prediction at Test-time}

\begin{algorithm}[tb]
\caption{Anytime Linear Prediction at Test-time}
\label{algo:anytime_linear}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} An ordering of features $S = s1, s2, ...$. 
The linear prediction weight $w(S_j)$ for each $j=1,2,...,$.
Input feature matrix $X$.
\STATE {\bfseries Output:} Linear prediction on $X$.
\STATE Initialize $\hat{Y}_0 = \vec{0}$. 
\STATE Initialize $\hat{Y} = \hat{Y}_0$. 
\FOR {$j = 1, 2, ...$}
	\STATE Compute feature group $s_j$.
	\STATE Compute predictions $\hat{Y}_j = Xw(S_j)$.
	\STATE Update $\hat{Y} = \hat{Y}_j$
\ENDFOR
\STATE Return $\hat{Y}$. 
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{algo:anytime_linear} describes anytime linear prediction at test-time. 
Given a learned ordering $S$ for computation the features, the predictor compute them in order
and update the linear prediction $\hat{Y}$ whenever a new feature becomes available. 
We can update predictions frequently, because we assume that the linear prediction computation
is dominated by its feature computation.
At interruption or termination of the feature computation, we report the latest linear prediction. 
Hence, to produce anytime linear predictions, we need to learn a sequencing of the features groups.


\subsection{Computation-Aware Group Orthogonal Matching Pursuit(CS-G-OMP)}


\begin{algorithm}[tb]
\caption{Cost Sensitive Group Orthogonal Matching Pursuit (CS-G-OMP)}
 \label{algo:gomp_lm}
\begin{algorithmic}[1]
	\STATE {\bfseries Input:} The normalized feature matrix $X \in \R^{n \times D}$.
 	The normalized response vector $Y \in \mathbb{R}^{n}$, which has a zero mean and unit variance. 
    Feature groups $\mathcal{G}_1, ... \mathcal{G}_J$ that partition $\{1,..,D\}$, and group costs $c(\mathcal{G}_j)$.
   Regularization constant $\lambda$.
	\STATE {\bfseries Output:} A sequence $G = g_1, g_2, ..., g_{J}$ of feature groups.
   For each $j \leq J$, a coefficient  $w(G_j)$ for the features $G_j = g_1,..., g_j$. 
    \STATE Set $G_0 = \emptyset$ to be an empty sequence.
    \STATE Set $w(G_0) = \vec{0}$ to be a zero vector of zero length.
    \STATE Compute $C = X^TX$. 
 \FOR {$j = 1,2,...,J$}
    \FOR {$g \notin G_{j-1}$}
        \STATE Compute $b_g = \nabla _g F(G_{j-1}) = \frac{1}{n} X_g^T(Y-X_{G_{j-1}}w(G_{j-1}))$.
        \label{algline:gomp_bg}
    \ENDFOR
    \STATE $g_{j} = \argmax \limits_{g = \mathcal{G}_1, ... , \mathcal{G}_J, g\notin G_{j-1}} \frac{b_g ^T (X_{g}^TX_g)^{-1}b_g}{ c(g) }$.
   \STATE Append $g_j$ to the sequence: $G_{j} = G_{j-1} \oplus g_{j}$.
   \STATE Compute $w(G_j) = \frac{1}{n}C_{G_{j}}^{-1}X_{G_{j}}^TY$.
 \ENDFOR
\end{algorithmic}
\end{algorithm}

 
In Algorithm~\ref{algo:gomp_lm}, we present Computation-Aware Group Orthogonal Matching Pursuit (CS-G-OMP), 
which learns a near-optimal sequencing of the feature groups for anytime linear predictions. 
The feature groups are selected greedily. At the $j^{th}$ selection step $(*)$, we have chosen $j-1$ groups,
$G_{j-1} = g_1, g_2, ..., g_{j-1}$, and have computed 
the best model using $G_{j-1}$, $w(G_{j-1})$. 
To evaluate a feature group $g$,
we first compute the gradient $b_g = \nabla _g F(G_{j-1})$ of the 
explained variance $F$ with respect to the coefficients of $g$.
Then, we evaluate it with the whitened gradient $L_2$-norm square per unit cost, 
$\frac{b_g ^T (X_{g}^TX_g)^{-1}b_g}{ c(g) }$. We select the group $g$ that 
maximizes this value as $g_j$, and continue until all groups
are depleted. 

Before providing performance guarantees with formal theoretical 
analysis of Algorithm~\ref{algo:gomp_lm} in Section~\ref{sec:gomp_proof}, we 
first provide some intuition on why we introduce a group-whitening at 
line~\ref{algline:gomp_bg} in Algorithm~\ref{algo:gomp_lm}.
If there are no feature groups, OMP greedily selects features whose
coefficients have the largest gradients of the objective function. 
In linear regression, the gradient for a feature $g$ 
is the inner-product of $X_g$ and the prediction 
residual $Y-\hat{Y}$. Hence OMP selects features that best reconstruct the 
residual. From this perspective, OMP under group setting 
should seek the feature group whose span contains the largest projection of the residual. 
Let the projection to feature group $g$ be 
$P_g = X_g(X_g^TX_g)^{-1}X_g^T$ and recall projection matrices are
idempotent. We observe that the criterion for CS-G-OMP selection step is
$\frac{\Vert P_g (Y - \hat{Y}) \Vert _2^2 }{c(g)}$, i.e, a cost-weighted
norm square of the projection of the residual onto a feature group. The 
name group whitening is chosen because the criterion is 
$\frac{\Vert b_g \Vert_2^2}{c(g)}$ if and only if feature groups are whitened. 
We assume \textit{feature groups are whitened} in our formal analysis, and we will
reflect on the theoretical effects of not group-whitening during the analysis.

%%%%%%%%%TODO 
Besides group-whitening, one may suggest other approaches 
to evaluate gradient vectors $b_g$ for group $g$. For example, 
$L_2$ norm and $L_{\infty}$ norm can be used to 
achieve greedy criteria $\frac{\Vert b_g \Vert_2^2}{c(g)}$ and 
$\frac{\Vert b_g \Vert ^2_{\infty}}{c(g)}$, respectively. 
The former criterion forgoes group whitening, so we call it \textit{no-whiten}.
Thus, it overestimates a feature group that has correlated 
but effective features, an extreme example of which is a
feature group of identical but effective features. The latter
criterion evaluates only the best feature of each feature group, so we call it \textit{single}. Thus, it
underestimates a feature group that has a descriptive 
feature span but no top-performing individual feature dimensions.
We will show in experiments that no-whiten and single
are indeed inferior to our CS-G-OMP choice. 


\subsection{Computation-Aware Group Forward Regression (CS-G-FR)}

The learning procedure extending from Forward Regression is similar to
Algorithm~\ref{algo:gomp_lm}, as stated in Algorithm~\ref{algo:gfr_lm}: we compute the linear models 
$w(G_{j-1} \oplus g)$ at line 4 instead of the 
gradients $b_g$ and replace the selection criterion $\frac{b_g ^T (X_{g}^TX_g)^{-1}b_g}{ c(g) }$ 
at line~\ref{algline:gomp_bg} with the marginal gain in explained 
variance per unit cost, $\frac{F(G_{j-1} \oplus g) - F(G_{j-1}) }{c(g)}$. 
We call this cost-sensitive FR extension as CS-G-FR.


\begin{algorithm}[tb]
\caption{Cost Sensitive Group Forward Regression (CS-G-OMP)}
 \label{algo:gfr_lm}
\begin{algorithmic}[1]
	\STATE {\bfseries Input:} The normalized feature matrix $X \in \R^{n \times D}$.
 	The normalized response vector $Y \in \mathbb{R}^{n}$, which has a zero mean and unit variance. 
    Feature groups $\mathcal{G}_1, ... \mathcal{G}_J$ that partition $\{1,..,D\}$, and group costs $c(\mathcal{G}_j)$.
   Regularization constant $\lambda$.
	\STATE {\bfseries Output:} A sequence $G = g_1, g_2, ..., g_{J}$ of feature groups.
   For each $j \leq J$, a coefficient  $w(G_j)$ for the features $G_j =g_1,..., g_j$. 
    \STATE Set $G_0 = \emptyset$ to be an empty sequence.
    \STATE Set $w(G_0) = \vec{0}$ to be a zero vector of zero length.
    \STATE Compute $C = X^TX$. 
 \FOR {$j = 1,2,...,J$}
    \FOR {$g \notin G_{j-1}$}
        \STATE Compute $w = w(G_{j-1} \oplus g) =  \frac{1}{n}C_{G_{j-1} \oplus g}^{-1}X_{G_{j-1} \oplus g}^TY$.
        \STATE Compute $F(G_{j-1} \oplus g) = \frac{1}{2n}(\Vert Y \Vert ^2 - \Vert Y - X_{G_{j-1} \oplus g} w \Vert ^2) 
        	- \frac{\lambda}{2} \Vert w \Vert ^2$. 
    \ENDFOR
    \STATE $g_{j} = \argmax \limits_{g = \mathcal{G}_1, ... , \mathcal{G}_J, g\notin G_{j-1}} 
	\frac{F(G_{j-1} \oplus g) - F(G_{j-1})}{c(g)}$.
   \STATE Append $g_j$ to the sequence: $G_{j} = G_{j-1} \oplus g_{j}$.
   \STATE Record $w(G_j) = \frac{1}{n}C_{G_{j}}^{-1}X_{G_{j}}^TY$.
 \ENDFOR
\end{algorithmic}
\end{algorithm}



%Note that we use $L_2$-norm square of the gradients of the groups, $\Vert b_g \Vert _2^2$, as a measurement of the effectiveness of a feature group. We show experimentally and theoretically that replacing $L_2$-norm with $L_{\infty}$ results in worse performance. We call the $L_{\infty}$ alternative as \textbf{Single}, since it evaluates a feature group based on its single best feature dimension. It is also crucial that 
%we assume each feature group $g$ to be whitened, i.e., $X_g^TX_g = I_{D_g}$, a property that we call \textbf{Group Whitened}. Theoretical analyses of \cite{gomp} and \cite{log_gomp} also rely on this assumption but did not explain the impact of not group whitening the data, which we will do in experiments and analysis. 
%As mentioned in \cite{log_gomp}, if the data is not group whitened before training, one equivalent method  of group whitening is to replace 
%$\Vert X_g^T (Y - X_{G_{j-1}}w) \Vert _2^2$ in step $(*)$ with 
%\begin{align}
%  &{b_g^{G_{j-1}}}^T(X_g^TX_g)^{-1}b_g^{G_{j-1}}=  \notag \\
%  & (Y - X_{G_{j-1}}w)^T X_g(X_g^TX_g)^{-1}X_g^T (Y - X_{G_{j-1}}w).
%\end{align}
%This replacement frees us from group whitening each individual sample, reducing training complexity by $O(nD^2)$ operations. 
