%\def\year{2019}\relax
%\documentclass[letterpaper]{article}
%\usepackage{aaai19}
%\usepackage{times}
%\usepackage{helvet}
%\usepackage{courier}
%\usepackage{url}
%\usepackage{graphicx}
%\frenchspacing 
%\setlength{\pdfpagewidth}{8.5in}
%\setlength{\pdfpageheight}{11in}
%\setcounter{secnumdepth}{2} 
%
%%PDF Info Is Required:
%  \pdfinfo{
%/Title (Learning Anytime Predictions in Neural Networks via Adaptive Loss Balancing)
%/Author (Hanzhang Hu, Debadeepta Dey, Martial Hebert, J. Andrew Bagnell)}
%
%\usepackage{subfig}
%\usepackage{xspace}
%\usepackage{color}
%\usepackage{longtable}
%\usepackage{hhline}
%
%% For algorithms
%\usepackage{algorithm}
%\usepackage{algorithmic}
%
%
%% Employ the following version of the ``usepackage'' statement for
%% submitting the draft version of the paper for review.  This will set
%% the note in the first column to ``Under review.  Do not distribute.''
%
%%\usepackage{fancyhdr}		% allows headers and footers
%%\usepackage{lastpage}		% provides page number of last page
%
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{amsthm}
%
%\makeatletter
%\newtheorem*{rep@theorem}{\rep@title}
%\newcommand{\newreptheorem}[2]{%
%\newenvironment{rep#1}[1]{%
% \def\rep@title{#2 \ref{##1}}%
% \begin{rep@theorem}}%
% {\end{rep@theorem}}}
%\makeatother
%
%\newreptheorem{theorem}{Theorem}
%\newreptheorem{definition}{Definition}
%
%
%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{assumption}[theorem]{Assumption}
%\newenvironment{myproof}[1][\proofname]{\proof[#1]\mbox{}}{\endproof}
%
%\newcommand{\annnames}{Anytime Neural Networks\xspace}
%\newcommand{\annname}{Anytime Neural Network\xspace}
%\newcommand{\ann}{ANN\xspace}
%\newcommand{\annnp}{ANN} 
%\newcommand{\anns}{ANNs\xspace}
%\newcommand{\aannnp}{AANN}
%\newcommand{\aann}{AANN\xspace}
%\newcommand{\aanns}{AANNs\xspace}
%\newcommand{\adaloss}{AdaLoss\xspace}
%\newcommand{\sieve}{SIEVE\xspace}
%\newcommand{\explin}{EXP-LIN\xspace}
%\newcommand{\const}{CONST\xspace}
%\newcommand{\linear}{LINEAR\xspace}
%\newcommand{\round}[1]{\lfloor #1 \rceil}
%
%\title{Learning Anytime Predictions in Neural Networks  
%via Adaptive Loss Balancing  }
%
%\author{
%Hanzhang Hu,\textsuperscript{\rm 1}
%Debadeepta Dey,\textsuperscript{\rm 2}
%Martial Hebert,\textsuperscript{\rm 1}
%J. Andrew Bagnell\textsuperscript{\rm 1}\\
%\textsuperscript{\rm 1}Carnegie Mellon University,
%\textsuperscript{\rm 2}Microsoft Research\\
%hanzhang@cs.cmu.edu, dedey@microsoft.com, hebert@cs.cmu.edu, dbagnell@cs.cmu.edu
%}
%
%
%\begin{document}
%\maketitle
%\begin{abstract}
%This work considers the trade-off between accuracy and test-time computational cost of deep neural networks (DNNs) via \emph{anytime} predictions from auxiliary predictions. Specifically, we optimize auxiliary losses jointly in an \emph{adaptive} weighted sum, where the weights are inversely proportional to average of each loss. 
%Intuitively, this balances the losses to have the same scale.
%We demonstrate theoretical considerations that motivate this approach from multiple viewpoints, including connecting it to optimizing the geometric mean of the expectation of each loss, an objective that ignores the scale of losses. 
%Experimentally, the adaptive weights induce more competitive anytime predictions on multiple recognition data-sets and models than non-adaptive approaches including weighing all losses equally. In particular, anytime neural networks (\anns) can achieve the same accuracy faster using adaptive weights on a small network than using static constant weights on a large one.
%For problems with high performance saturation, we also show a sequence of exponentially deepening \anns can achieve near-optimal anytime results at any budget, at the cost of a const fraction of extra computation. 
%\end{abstract}
%
%

\section{Introduction}
\label{sec:introduction}

Recent years have seen advancement in visual recognition tasks
by increasingly accurate convolutional neural networks, from AlexNet~\cite{alexnet} and VGG~\cite{vgg}, to ResNet~\cite{resnet}, ResNeXt~\cite{resnext}, and DenseNet~\cite{densenet}. 
As models become more accurate and computationally expensive, it becomes more difficult for applications to choose between slow predictors with high accuracy and fast predictors with low accuracy. 
Some applications also desire multiple trade-offs between computation and accuracy, because they have computational budgets that may vary at test time. E.g., web servers for facial recognition or spam filtering may have higher load during the afternoon than at midnight.  Autonomous vehicles need faster object detection when moving rapidly than when it is stationary.  Furthermore, real-time and latency sensitive applications may desire fast predictions on easy samples and slow but accurate predictions on difficult ones. 
\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth, keepaspectratio]{\ANNDIR/adaloss_small_vs_const_large.png}
    \caption{The common \ann training strategy increases final errors from the optimal (green vs. blue), which decreases exponentially slowly. By learning to focus more on the final auxiliary losses, the proposed adaptive loss weights make a small \ann (orange) to outperform a large one (green) that has non-adaptive weights. }
    \label{fig:sieve_small_vs_const_large}
\end{figure}

An \textbf{anytime predictor}~\cite{horvitz:1987,boddydean,anytime,speedboost,msdense} can automatically trade off between computation and accuracy. For each test sample, an anytime predictor produces a fast and crude initial prediction and continues to refine it as budget allows, so that at any test-time budget, the anytime predictor has a valid result for the sample, and the more budget is spent, the better the prediction. 
Anytime predictors are different from cascaded predictors~\cite{cascade,xu:14,cai:15,adaptivenn,cascade_nn} for \textbf{budgeted prediction}, which aim to minimize \textbf{average test-time computational cost} without sacrificing average accuracy: a different task (with relation to anytime prediction). Cascades achieve this by early exiting on easy samples to save computation for difficult ones, but cascades cannot incrementally improve individual samples after an exit. Furthermore, early exit policy of cascades can be combined with existing anytime predictors~\cite{adaptivenn,cascade_nn}. Hence, we consider cascades to be orthogonal to anytime predictions.




\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth, keepaspectratio]{\ANNDIR/ann.png}
    \caption{Anytime neural networks contain auxiliary predictions and losses, $\hat{y}_i$ and $\ell_i$, for intermediate feature unit $f_i$.  }
    \label{fig:ann}
\end{figure}

This work studies how to convert well-known DNN architectures to produce competitive anytime predictions.  
We form anytime neural networks (\anns) by appending auxiliary predictions and losses to DNNs, as we will detail in Sec.~\ref{sec:ann} and Fig.~\ref{fig:ann}. Inference-time prediction then can be stopped at the latest prediction layer that is within the budget. Note that this work deals with the case where it is \textbf{not known apriori} where the interrupt during inference time will occur. 
We define the optimal at each auxiliary loss as the result from training the \ann only for that loss to convergence. Then our objective is to have near-optimal final predictions and competitive early ones.  Near-optimal final accuracy is imperative for anytime predictors, because, as demonstrated in Fig.~\ref{fig:sieve_small_vs_const_large}, accuracy gains are often exponentially more expensive as model sizes grow, so that reducing 1\% error rate could take 50\% extra computation. Unfortunately, existing anytime predictors often optimize the anytime losses in static weighted sums~\cite{supervisednet,feedbacknet,msdense} that poorly optimize final predictions, as we will show in Sec.~\ref{sec:multi_objective} and Sec.~\ref{sec:experiment_questions}.

Instead, we optimize the losses in an \textbf{adaptive} weighted sum, where the weight of a loss is inversely proportional to the empirical mean of the loss on the training set. Intuitively, this normalizes losses to have the same scale, so that the optimization leads each loss to be about the same relative to its optimal. We provide multiple theoretical considerations to motivate such weights.
First of all, when the losses are mean square errors, our approach is maximizing the likelihood of a model where the prediction targets have Gaussian noises. Secondly, inspired by the maximum likelihood estimation, we optimize the model parameters and the loss weights jointly, with log-barriers on the weights to avoid the trivial solution of zero weights. Finally, we find the joint optimization equivalent to optimizing the geometric mean of the expected training losses, an objective that treats the relative improvement of each loss equally. Empirically, we show on multiple models and visual recognition data-sets that the proposed adaptive weights outperform natural, non-adaptive weighting schemes as follows.
We compare small \anns using our adaptive weights against \anns that are $50\sim 100\%$ larger but use non-adaptive weights. The small \anns can reach the same final accuracy as the larger ones, and reach each accuracy level faster.

Early and late accuracy in an \ann are often anti-correlated (e.g., Fig.~7 in~\cite{msdense} shows \anns with better final predictions have worse early ones). To mitigate this \emph{fundamental} issue we propose to assemble \anns of exponentially increasing depths. If \anns are near-optimal in a late fraction of their layers, the exponential ensemble only pays a constant fraction of additional computation to be near-optimal at every test-time budget. In addition, exponential ensembles outperform linear ensembles of networks, which are commonly used baselines for existing works~\cite{feedbacknet,msdense}. 
In summary our contributions are:
\begin{itemize}
    %\item We highlight the importance of near-optimal final predictions in anytime models.
    \item We derive an adaptive weight scheme for training losses in \anns from multiple theoretical considerations, and show that experimentally this scheme achieves near-optimal final accuracy \emph{and} competitive anytime ones on multiple data-sets and models.
    \item We assemble \anns of exponentially increasing depths to achieve near-optimal anytime predictions at every budget at the cost of a constant fraction of additional consumed budget. 
\end{itemize}



\section{Related Works}
\label{sec:background}


\textbf{Meta-algorithms for anytime and budgeted prediction.}
Anytime and budgeted prediction has a rich history in learning literature.
\cite{weinberger09feature,xu:12,xu:13b} sequentially generate features to empower the final predictor.
\cite{reyzin:11,speedboost,hu:16} apply boosting and greedy methods to order feature and predictor computation.  
\cite{timeliness,rl_anytime} form Markov Decision Processes for computation of weak predictors and features, and learn policies to order them. However, these meta-algorithms are not easily compatible with complex and accurate predictors like DNNs, because the anytime predictions without DNNs are inaccurate, and there are no intermediate results during the computation of the DNNs.
Cascade designs for budgeted prediction \cite{cascade,lefakis:10,chen:12,xu:14,cai:15,adaptive_select,adaptivenn,cascade_nn} reduce the average test-time computation by early exiting on easy samples and saving computation for difficult ones. As cascades build upon existing anytime predictors, or combine multiple predictors, they are orthogonal to learning \anns end-to-end.

\textbf{Neural networks with early auxiliary predictions.} 
Multiple works have addressed training DNNs with early auxiliary predictions for various purposes. \cite{supervisednet,inception_v4,pspnet,fractalnet} use them to regularize the networks for faster and better convergence. \cite{curriculum,feedbacknet} set the auxiliary predictions from easy to hard for curriculum learning. \cite{hed,reverse_scene_seg} make pixel level predictions in images, and find learning early predictions in coarse scales also improve the fine resolution predictions. \cite{msdense} shows the crucial importance of maintaining multi-scale features for high quality early classifications. The above works use manually-tuned static weights to combine the auxiliary losses, or change the weights only once~\cite{reverse_scene_seg}. This work proposes adaptive weights to balance the losses to the same scales online, and provides multiple theoretical motivations. We empirically show adaptive losses induce better \anns on multiple models, including the state-of-the-art anytime predictor for image recognition, MSDNet~\cite{msdense}. %There also exist works that train auxiliary losses stage-wise~\cite{boostresnet}, but they suffer from worse accuracy generally. 

\textbf{Model compression.} 
Many  works have studied how to compress neural networks.  \cite{prune_nn,slim_nn} prune network weights and connections. \cite{binary_nn,binary_nn_eccv,squeezenet} quantize weights within networks to reduce computation and memory footprint. 
\cite{wang2017skipnet,veit2017convolutional} dynamically skip network computation based on samples.
\cite{deepreally,distillation} transfer knowledge of deep networks into shallow ones by changing the training target of shallow networks. 
These works are orthogonal to ours, because they train a separate model for each trade-off between computation and accuracy, but we train a single model to handle all possible trade-offs.



%#########################################################
%               Methods
%#########################################################

\section{Optimizing Anytime Predictors}

%\subsection{Training Anytime Neural Networks via Multi-objective Optimization}
\label{sec:ann}

%\subsection{Anytime Augmentation to a Feed-forward Network}
%\label{sec:ann-struct}




%given a sample $(x,y)\sim D$, the initial feature map $x_0$ is set to $x$, and the subsequent feature transformations $f_1, f_2,..., f_L$ generate a sequence of intermediate features $x_i = f_i(x_{i-1} ; \theta_i)$ for $i\geq 1$ using parameter $\theta_i$.  Each feature map $x_i$ can then produce an auxiliary prediction $\hat{y}_i$ using a prediction layer $g_i$: $\hat{y}_i = g_i(x_i; w_i)$ with parameter $w_i$. Each auxiliary prediction $\hat{y}_i$ then incurs an expected loss $\ell_i := E _{(x,y) \sim D} [ \ell( y, \hat{y}_i)]$. We call such an augmented network as an \textit{\annname (\ann)}. 


%For instance, if we base ANNs on ResNets \cite{resnet}, then each $f_i$ contains $s$ number of residual building blocks (whose implementation details are deferred to the appendix), where $s$ is the prediction period. Each $g_i$ outputs the predicted label distribution $\hat{y}_i$ using a global pooling layer followed by a linear layer and a softmax; the loss function $\ell$ is the cross-entropy loss.
%During test-time, an \ann computes $\hat{y}_i$ sequentially until interruption from either users or early exit policies. %The above augmentation is general to feed-forward convolutional network as we experiment in Sec.~\ref{sec:other_networks}.  



%since complex neural networks can fit any labeling of the data for our available data-sets \cite{bengio2016}, 
%when the network is highly over-parameterized, it may be reasonable to assume that the sub neural networks of the full network can encode extra information to enable competitive early predictions, and there may exist $\theta$ such that $\ell_i (\theta) \approx \ell_i^*$ for any $i$.  


%Let the parameters of the full \ann be $\theta = (\theta_1, w_1,  ..., \theta_L, w_L)$. Let $\ell^*_i := \min _{\theta} \ell_i(\theta)$ be the optimal loss at the $i^{th}$  prediction. The goal of training an \ann is to find a single $\theta^*$ such that $\ell_i(\theta^*) =\ell^*_i$ for all $i$, i.e., 
%$ \theta^* \in \cap _{i=1}^L \{ x : x = \arg \min _{\theta} \ell _i(\theta) \}. \label{eq:multi-objective}$
% If the above multi-objective optimization can be solved, then there exists a $\theta^*$ such that simultaneously optimizes all $\ell_i$, i.e., $\ell_i(\theta^*) = \ell_i^*$ for any $i$. 


\label{sec:multi_objective}

As illustrated in Fig.~\ref{fig:ann}, a feed-forward network consists of a sequence of transformations $f_1,...,f_L$ of feature maps. Starting with the input feature map $x_0$, each subsequent feature map is generated by $x_i = f_i(x_{i-1})$.  Typical DNNs use the final feature map $x_L$ to produce predictions, and hence require the completion of the whole network for results. Anytime neural networks (\anns) instead introduce auxiliary predictions and losses using the intermediate feature maps $x_1,...,x_{L-1}$, and thus, have early predictions that are improving with computation. 


%%%%%%%%%%%%%%%%%


\textbf{Weighted sum objective.} Let the intermediate predictions be $\hat{y}_i = g_i(x_i)$ for some function $g_i$, and let the corresponding expected loss be $\ell_i = E_{(x_0,y)\sim \mathcal{D}} [\ell(y, \hat{y}_i)]$, where $\mathcal{D}$ is the distribution of the data, and $\ell$ is some loss such as cross-entropy.  Let $\theta$ be the parameter of the \ann, and define the optimal loss at prediction $\hat{y}_i$ to be $\ell_i* = \min _{\theta}  \ell_i(\theta)$. Then the goal of anytime prediction is to seek a universal 
$
    \theta^* \in \cap _{i=1}^L \{ \theta' : \theta' = \arg \min _{\theta} \ell _i(\theta) \}.
    \label{eq:multi-objective}
$
Such an ideal $\theta^*$ does not exist in general as this is a multi-objective optimization, which only has Pareto front, a set containing all solutions such that improving one $\ell_i$ necessitates degrading others. Finding all solutions in the Pareto front for \anns is not practical or useful, since this requires training multiple models, but each \ann only runs one. Hence, following previous works on anytime models~\cite{supervisednet,feedbacknet,msdense}, we optimize the losses in a weighted sum
$
    \min _{\theta} \sum _{i=1}^L B_i \ell_i (\theta),
    \label{eq:sum-objective}
$
where $B_i$ is the weight of the loss $\ell_i$. We call the choices of $B_i$ \textit{weight schemes}. 


%%%%%%%%%%%%%%%%%

\textbf{Static weight schemes.} Previous works often use static weight schemes as part of their formulation. \cite{supervisednet,hed,msdense} use \const scheme that sets $B_i =1$ for all $i$. \cite{feedbacknet} use \linear scheme that sets $B_1$ to $B_L$ to linearly increase from $0.25$ to $1$. However, as we will show in Sec.~\ref{sec:compare_opt}, these static schemes not only cannot adjust weights in a data and model-dependent manner, but also may significantly degrade predictions at later layers.

\begin{figure}
    \centering
        \includegraphics[width=0.80\linewidth, keepaspectratio]{\ANNDIR/loss_cifar100.png}
    \caption{Relative Percentage Increase in Training Loss vs. depths (lower is better). \const scheme is increasingly worse than the optimal at deep layers. \adaloss performs about equally well on all layers in comparison to the OPT.}
    \label{fig:loss_cifar100}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth, keepaspectratio]{\ANNDIR/EANN.png}
    \caption{ Ensemble of exponentially deepening anytime neural network (EANN) computes its \anns in order of their depths. An anytime result is used if it is better than all previous ones on a validation set (layers in light blue).} 
     \label{fig:eann}
\end{figure}
    %}    
    
%%%%%%%%%%%%%%


\textbf{Qualitative weight scheme comparison.} Before we formally introduce our proposed adaptive weights, we first shed light on how existing static weights suffer. We experiment with a ResNet of 15 basic residual blocks on CIFAR100~\cite{cifar} data-set (See Sec.~\ref{sec:experiment_questions} for data-set details). An anytime predictor is attached to each residual block, and we estimate the optimal performance (OPT) in training cross entropy of predictor $i$ by training a network that has weight only on $\ell_i$ to convergence. Then for each weight scheme we train an \ann to measure the relative increase in training loss at each depth $i$ from the OPT. In Fig.~\ref{fig:loss_cifar100}, we observe that the intuitive \const scheme has high relative losses in late layers. This indicates that there is not enough weights in the late layers, though losses have the same $B_i$.  We also note that balancing the weights is non-trivial. For instance, if we put half of the total weights in the final layer and distribute the other half evenly, we get the ``Half-End" scheme. As expected, the final loss is improved, but this is at the cost of significant increases of early training losses. In contrast, the adaptive weight scheme that we propose next (\adaloss), achieves roughly even relative increases in training losses automatically, and is much better than the \const scheme in the late layers.

%%%%%%%%%%%%%%

\textbf{Adaptive Loss Balancing (\adaloss).} 
% Why the auxiliary losses should have different weights when they are all cross-entropy losses between predictions and the target label? 
Given all losses are of the same form (cross-entropy), it may be surprising
that better performance is achieved with differing weights.
Because early features typically have less predictive power than later ones, early losses are naturally on a larger scale and possess larger gradients. Hence, if we weigh losses equally, early losses and gradients often dominate later ones, and the optimization becomes focused on the early losses.  
To automatically balance the weights among the losses of different scales, we propose an adaptive loss balancing scheme (\adaloss). Specifically, we keep an exponential average of each loss $\hat{\ell}_i$ during training, and set $B_i \propto  \frac{1}{\hat{\ell}_i}$. This is inspired by \cite{reverse_scene_seg}, which scales the losses to the same scale \emph{only once} during training, and provides a brief intuitive argument: the adaptive weights set the losses to be on the same scale. 
We next present multiple theoretical justifications for \adaloss.
 
Before considering general cases, we first consider a simple example, where the loss function $\ell(y, \hat{y}) = \Vert y - \hat{y}\Vert^2_2$ is the square loss. For this example, we model each $y|x$ to be sampled from the multiplication of $L$ independent Gaussian distributions, $\mathcal{N}(\hat{y}_i, \sigma_i^2 I)$ for $i=1,...,L$, where $\hat{y}_i(x; \theta)$ is the $i^{th}$ prediction, and $\sigma_i^2 \in \mathbb{R}^+$, i.e., 
$Pr(y|x ; \theta, \sigma_1^2,..., \sigma_L^2) \propto \prod _{i=1}^L  \frac{1}{\sqrt{\sigma_i^2}}\exp(-\frac{\Vert y - \hat{y}_i\Vert_2^2 }{2 \sigma_i^2})$. 
Then we compute the empirical expected log-likelihood for a maximum likelihood estimator (MLE):
\begin{align}
\hat{E}\big[\ln (Pr(y|x))\big]
&\propto  \hat{E} \big[\sum _{i=1}^L( -\frac{\Vert y - \hat{y}_i\Vert^2_2}{\sigma_i^2} -  \ln \sigma_i^2 ) \big] \\
&= \sum _{i=1}^L ( -\frac{\tilde{\ell}_i }{\sigma_i^2} -  \ln \sigma_i^2 ),
\label{eq:mle_to_log_barrier}
\end{align}
where $\hat{E}$ is averaging over samples, and $\tilde{\ell}_i$ is the empirical estimate of $\ell_i$. If we fix $\theta$ and optimize over $\sigma_i^2$, we get $\sigma_i^2 = \tilde{\ell}_i$.
As computing the empirical means is expensive over large data-sets, \adaloss replaces $\tilde{\ell}_i$ with $\hat{\ell}_i$, the exponential moving average of the losses, and sets $B_i \propto \hat{\ell}_i^{-1} \approx \sigma_i^{-2}$ so as to solve the MLE online by jointly updating $\theta$ and $B_i$. We note that the naturally appeared $\ln \sigma_i^2$ terms in Eq.~\ref{eq:mle_to_log_barrier} are log-barriers preventing $B_i=0$. 

Inspired by this observation, we form the following joint optimization over $\theta$ and $B_i$ for general losses without probability models:
\begin{align}
    \min _{\theta, B_1,...,B_L} \sum _{i=1}^L (B_i \ell _i(\theta) - \lambda \ln B_i),
    \label{eq:weighted_sum_log_barrier}
\end{align}
where $\lambda > 0$ is a hyper parameter to balance between the log-barriers and weighted losses. Under the optimal condition, $B_i=\frac{\lambda}{\ell_i}$. \adaloss estimates this with $B_i \propto \hat{\ell}_i(\theta)^{-1}$.  
We can also eliminate $B_i$ from Eq.~\ref{eq:weighted_sum_log_barrier} under the optimal condition, and we transform Eq.~\ref{eq:weighted_sum_log_barrier} to the following problem:
\begin{align}
    \min _{\theta} \sum _{i=1}^L \ln \ell _i (\theta).
    \label{eq:geometric_mean}
\end{align}
This is equivalent to minimizing the geometric mean of the expected training losses, and it differs from minimizing the expected geometric mean of losses, as $\ln$ and expectation are not commutable. 
Eq.~\ref{eq:geometric_mean} discards any constant scaling of losses automatically discarded as constant offsets, so that the scale difference between the early and late losses are automatically reconciled. Geometric mean is also known as the canonical mean for multiple positive quantities of various scales. \adaloss optimizes  Eq.~\ref{eq:geometric_mean}, since the objective gradient is $\sum _{i=1}^L \frac{ \nabla \ell _i (\theta)} {\ell_i (\theta)}$. \adaloss wants to weigh each $\ell_i(\theta)$ by exactly $\frac{1}{{\ell}_i (\theta) }$, and estimates the weight by $\frac{1}{\hat {\ell}_i (\theta) }$.
This concludes our theoretical considerations for \adaloss.



%%%%%%%%%%%%%%%%




\section{Sequence of Exponentially Deepening Anytime Neural Networks (EANN)}
\label{sec:eann}


In practice, we often observe \anns using \adaloss to be much more competitive in their later half than the early half on validation sets, such as in Table.~\ref{tab:compare_f} of Sec.~\ref{sec:compare_opt}. Fortunately, we can leverage this effect to form competitive anytime predictors at every budget, with a constant fraction of additional computation. Specifically, we assemble \anns whose depths grow exponentially. Each \ann only starts computing if the smaller ones are finished, and its predictions are used if they are better than the best existing ones in validation. We call this ensemble an \textbf{EANN}, as illustrated in Fig.~\ref{fig:eann}. An EANN only delays the computation of any large \ann by at most a constant fraction of computation, because the earlier networks are exponentially smaller. Hence, if each \ann is near-optimal in later predictions, then we can achieve near-optimal accuracy at any test-time interruption, with the extra computation. 
Formally, the following proposition characterizes the exponential base and the increased computational cost.  


\begin{proposition}
Let $b>1$. Assume for any $L$, any \ann of depth $L$ has competitive anytime prediction at  depth $i > \frac{L}{b}$ against the optimal of depth $i$. Then after $B$ layers of computation, EANN produces anytime predictions that are competitive against the optimal of depth $\frac{B}{C}$ for some $C > 1$, such that $\sup _B C = 2+ \frac{1}{b-1}$, and $C$ has expectation
$E_{B\sim uniform(1,L)}[C] \leq 1 - \frac{1}{2b} + \frac{1 + \ln (b)}{b-1}$.
\label{prop:eann}
\end{proposition}
This proposition says that an EANN is competitive at any budget $B$ against the optimal of the cost $\frac{B}{C}$. Furthermore, the stronger each anytime model is, i.e., the larger $b$ becomes, the smaller the computation inflation, $C$, is: as $b$ approaches $\infty$, $\sup  _B C$, shrinks to 2, and $E[C]$, shrinks to 1.
Moreover, if we have $M$ number of parallel workers instead of one, we can speed up EANNs by computing \anns in parallel in a first-in-first-out schedule, so that we effectively increase the constant $b$ to $b^M$ for computing $C$. It is also worth noting that if we form the sequence using regular networks instead of \anns, then we will lose the ability to output frequently, since at budget $B$, we only produce $\Theta(\log(B))$ intermediate predictions instead of the $\Theta(B)$ predictions in an EANN. We will further have a larger cost inflation, $C$, such that $\sup  _B C \geq 4$ and $E[C] \geq 1.5 + \sqrt{2} \approx 2.91$, so that the average cost inflation is at least about $2.91$.
We defer the proofs to the appendix.




%#############################  ############################
%               EXPERIMENTS
%#############################  ############################
\section{Experiments}
\label{sec:experiment_questions}

We list the key questions that our experiments aim to answer.
\begin{itemize}

\item How do anytime predictions trained with adaptive weights compare against those trained with static constant weights (over different architectures)?
(Sec.~\ref{sec:compare_opt})
\item How do underlying DNN architectures affect \anns? (Sec.~\ref{sec:compare_opt})
\item How can sub-par early predictions in \anns be mitigated by \ann ensembles? 
(Sec.~\ref{sec:eann_experiment})
\item How does data-set difficulty affect the adaptive weights scheme?
(Sec.~\ref{sec:weight_vs_dataset})
\end{itemize}


\subsection{Data-sets and Training Details}
\label{sec:exp}

\textbf{Data-sets.} We experiment on CIFAR10, CIFAR100~\cite{cifar}, SVHN~\cite{svhn}\footnote{Both CIFAR data-sets consist of 32x32 colored images. CIFAR10 and CIFAR100 have 10 and 100 classes, and each have 50000 training and 10000 testing images. We held out the last 5000 training samples in CIFAR10 and CIFAR100 for validation; the same parameters are then used in other models. We adopt the standard augmentation from \cite{supervisednet,resnet}.
SVHN contains around 600000 training and around 26032 testing 32x32 images of numeric digits from the Google Street Views. We adopt the same pad-and-crop augmentations of CIFAR for SVHN, and also add Gaussian blur.}
and ILSVRC~\cite{ILSVRC15}\footnote{
ILSVRC2012~\cite{ILSVRC15} is a visual recognition data-set containing around 1.2 million natural and 50000 validation images for 1000 classes. We report the top-1 error rates on the validation set using a single-crop of size 224x224, after scaling the smaller side of the image to 256, following~\cite{resnet}.}.

\textbf{Training details.} We optimize the models using stochastic gradient descent, with initial learning rate of 0.1, momentum of 0.9 and a weight decay of 1e-4. On CIFAR and SVHN, we divide the learning rate by 10 at 1/2 and 3/4 of the total epochs. We train for 300 epochs on CIFAR and 60 epochs on SVHN.  On ILSVRC, we train for 90 epochs, and divide the learning rate by 10 at epoch 30 and 60. We evaluate test error using single-crop.


\textbf{Base models.} We compare our proposed \adaloss weights against the intuitive \const weights. On CIFAR and SVHN, we also compare \adaloss against LINEAR and OPT, defined in Sec.~\ref{sec:multi_objective}. 
We evaluate the weights on multiple models including ResNet~\cite{resnet} and DenseNet~\cite{densenet}, and MSDNet~\cite{msdense}. For ResNet and DenseNet, we augment them with auxiliary predictors and losses, and call the resulting models Res\ann and Dense\ann, and defer the details of these models to the appendix Sec.~\ref{sec:implementation_ann}.


\subsection{Weight Scheme Comparisons}
\label{sec:compare_opt}


\begin{table}
    \centering
    \begin{tabular}{c|cccc}
    \hline
     & 1/4 & 1/2 & 3/4 & 1 \\
    \hline
    OPT
	&  0.00 &  0.00 &  0.00 &  0.00 \\
    CONST
	& \textbf{15.07} & 16.40 & 18.76 & 18.90 \\
    LINEAR
	& 25.67 & 13.02 & 12.97 & 12.65 \\
    ADALOSS
 & 32.99 &  \textbf{9.97} &  \textbf{3.96} &  \textbf{2.73} \\
    \hline
    \end{tabular}
        
    \caption{Average relative percentage increase in error from the OPT on CIFAR and SVHN at 1/4, 1/2, 3/4 and 1 of the total cost. E.g., the bottom right entry means that if OPT has a 10\% final error rate, then \adaloss has about 10.27\%.}
    \label{tab:compare_f}
\end{table}

\begin{table}
        \begin{tabular}{c|cccc}
        \hline
         & 1/4 & 1/2 & 3/4 & 1 \\
        \hline
        Res\annnp50+C
    	& \textbf{54.34} & 35.61 & 27.23 & 25.14 \\
        Res\annnp50+A
    	& 54.98 & \textbf{34.92} & \textbf{26.59} & \textbf{24.42} \\
    	\hline
        Dense\annnp169+C  % exp 4007
    	& 48.15 & 45.00 & 29.09 & 25.60 \\
        Dense\annnp169+A % exp 3544
    	& \textbf{47.17} & \textbf{44.64} & \textbf{28.22} & \textbf{24.07} \\
        \hline      
        MSDNet38 
        & \textbf{33.9} & 28.0 & 25.7 & 24.3 \\
        MSDNet38+A % 4113
        & 35.75 & 28.04 & 25.82 & \textbf{23.99} \\
        \hline
        \end{tabular}
        
    
    \caption{ Test error rates at different fraction of the total costs on Res\annnp50, Dense\annnp169, and MSDNet38 on ILSVRC. The post-fix +C and +A stand for \const and \adaloss respectively. Published results of MSDNet38~\cite{msdense} uses \const.
    }
    \label{tab:compare_f_ilsvrc}
\end{table}



\begin{figure*}[t]
    \centering
    
    \subfloat[Res\anns on CIFAR10]{
        \includegraphics[width=0.32\linewidth, keepaspectratio ]{\ANNDIR/adaloss_vs_const_of_double_cost_cifar10.png}
        \label{fig:adaloss_vs_const_of_double_cost_cifar10}
    }
    ~
    \subfloat[Res\anns on CIFAR100]{
        \includegraphics[width=0.32\linewidth, keepaspectratio ]{\ANNDIR/adaloss_vs_const_of_double_cost_cifar100.png}
        \label{fig:adaloss_vs_const_of_double_cost_cifar100}
    }
    ~
    \subfloat[Res\anns on SVHN]{
        \includegraphics[width=0.32\linewidth, keepaspectratio ]{\ANNDIR/adaloss_vs_const_of_double_cost_svhn.png}
        \label{fig:adaloss_vs_const_of_double_cost_svhn}
    }
    
    \subfloat[Res\anns on ILSVRC]{
        \includegraphics[width=0.32\linewidth, keepaspectratio]{\ANNDIR/ilsvrc_adaloss_vs_const_resnet.png}
        \label{fig:ilsvrc_adaloss_vs_const_of_double_cost}
    }
    ~
    \subfloat[MSDNet on ILSVRC]{
        \includegraphics[width=0.32\linewidth, keepaspectratio]{\ANNDIR/ilsvrc_adaloss_vs_const_msdnet.png}
        \label{fig:ilsvrc_adaloss_vs_const_of_double_cost_msdnet}    
    }
    ~
    \subfloat[\anns comparison on ILSVRC]{
        \includegraphics[width=0.32\linewidth, keepaspectratio]{\ANNDIR/ilsvrc_compare_models.png}
        \label{fig:ilsvrc_compare_models}    
    }
    
    \caption{\textbf{(a-e)} Comparing small networks with \adaloss versus big ones using \const. With \adaloss, 
    the small networks achieve the same accuracy levels faster than large networks with \const. 
    \textbf{(f)} \anns performance are mostly decided by underlying models, but \adaloss is beneficial regardless models. }
    \label{fig:adaloss_vs_const_of_double_cost}
\end{figure*}
\textbf{\adaloss vs. \const on the same models.} Table~\ref{tab:compare_f} presents the average relative test error rate increase from OPT on 12  Res\anns on CIFAR10, CIFAR100 and SVHN\footnote{The 12 models are named by $(n,c)$ drawn from $\{ 7, 9, 13, 17, 25 \} \times \{ 16, 32 \}$ and $\{(9,64), (9,128)\}$, where $n$ represents the number of residual units in each of the three blocks of the network, and $c$ is the filter size of the first convolution.}. As training an OPT for each depth is too expensive, we instead report the average relative comparison at 1/4, 1/2, 3/4, and 1 of the total \ann costs. 
We observe that the \const scheme makes $15\sim 18\%$ more errors than the OPT, and the relative gap widens at later layers.  The \linear scheme also has about 13\% relative gap in later layers. In contrast, \adaloss enjoys small performance gaps in the later half of layers. 
On ILSVRC, we compare \adaloss against \const on Res\annnp50, Dense\annnp169, and MSDNet38, which have similar final errors and total computational costs (See Fig.~\ref{fig:ilsvrc_compare_models}). In Table~\ref{tab:compare_f_ilsvrc}, we observe the trade-offs between early and late accuracy on Res\annnp50 and MSDNet38. Furthermore, Dense\annnp169 performs \emph{uniformly} better with \adaloss than with \const. 
Since comparing the weight schemes requires evaluating \anns at multiple budget limits, and \adaloss and \const outperform each other at a significant fraction of depths on most of our experiments, we consider the two schemes \emph{incomparable on the same model}. 
%However, our next experiments will show late predictions to be vastly more important than the early ones.

\begin{figure*}[t]
    \centering
    \subfloat[EANNs on CIFAR100]{
        \includegraphics[width=0.32\linewidth, keepaspectratio]{\ANNDIR/cifar100_adaloss_eann_b2.png}
        \label{fig:eann_f}
    }
    ~
    \subfloat[EANN on ILSVRC]{
        \includegraphics[width=0.32\linewidth, keepaspectratio]{\ANNDIR/eann_comparisons.png}
        \label{fig:compare_ensemble}
    }
    ~
    \subfloat[AdaLoss Weights on three data-sets]{
    \includegraphics[width=0.32\linewidth, keepaspectratio]{\ANNDIR/adaloss_weights.png}
    \label{fig:adaloss_weights}
    }
    \caption{ \textbf{(a)} EANN performs better if the \anns use \adaloss instead of \const. 
    \textbf{(b)} EANN outperforms linear ensembles of DNNs on ILSVRC.
    \textbf{(c)} The learned adaptive weights of the same model on three data-sets.
    }
    %AANN-14 finishes at 33.50\% error,
\end{figure*}


\textbf{Small networks with \adaloss vs. large ones with \const.} 
Our previous comparison between \adaloss and \const on the same models is not fully conclusive, since each scheme can outperform the other at a significant portion of the total cost. To address this, we set the final error rate, model architecture type, and the filter size $c$ as constants, and vary the model depths so that \adaloss and \const reach the target final error rate. Then we compare the early predictions and the costs of models. 
On each of CIFAR10, 100 and SVHN, we compare six pairs of Res\anns, where the \const uses twice the computation as \adaloss\footnote{\adaloss takes $(n,c)$ from $\{7,9,13\} \times \{16, 32\}$, and \const takes $(n,c)$ from $\{13,17,25\} \times \{16, 32\}$.}. Fig.~\ref{fig:adaloss_vs_const_of_double_cost_cifar10},~\ref{fig:adaloss_vs_const_of_double_cost_cifar100}, and~\ref{fig:adaloss_vs_const_of_double_cost_svhn} show the averaged relative comparisons\footnote{The relative plots pivot at the final predictor from \adaloss, e.g., the location (0.5, 200) means having half the computation and 200\% extra relative errors than the final predictor from \adaloss}, and they show that the small \anns with \adaloss are better anytime predictors than the large ones with \const, because both models have the same final accuracy (on CIFAR10, the small ones are even better), and the small models reach the same error rates faster than the large ones. We have similar observations on ILSVRC using Res\anns and MSDNets in Fig.~\ref{fig:ilsvrc_adaloss_vs_const_of_double_cost} and Fig.~\ref{fig:ilsvrc_adaloss_vs_const_of_double_cost_msdnet}.
For instance, MSDNet~\cite{msdense} is the state-of-the-art anytime predictor. The published MSDNet38 uses \const, and has 24.3\% error rate using 6.6e9 total FLOPS in convolutions. By switching to \adaloss, we improve a much smaller MSDNet33 (details in the appendix), which costs 4.5e9 FLOPS, to reach 24.5\% final error. The two models also have similar early errors. 

\adaloss can reach the same accuracies with similar or smaller costs than \const, because in practice, a linear decrease in final error rate may often require an exponential increase in total computation, and \const degrades the final performances significantly (Table~\ref{tab:compare_f}). Since \adaloss requires much smaller models than \const to reach the same final errors, and with a fixed final error rate, \adaloss reaches each early error rate with less or similar cost, we conclude that \adaloss is the better scheme for anytime predictions. 

\textbf{Various base networks on ILSVRC.} We compare Res\anns, Dense\anns and MSDNets that have final error rate of near 24\% in Fig.~\ref{fig:ilsvrc_compare_models}, and observe that the anytime performance is mostly decided by the specific underlying model. MSDNets are more cost-effective than Dense\anns, which in turn are better than Res\anns. 
However, \adaloss is helpful regardless of underlying model. Both Res\annnp50 and Dense\annnp169 see improvements switching from \const to \adaloss, which is also shown in Table~\ref{tab:compare_f_ilsvrc}. 
Thanks to \adaloss, Dense\annnp169 achieves the same final error using similar FLOPS as the original published results of MSDNet38~\cite{msdense}. This suggests that~\cite{msdense} improve over Dense\anns by having better early predictions without sacrificing the final cost efficiency via impressive architecture insight. \adaloss brings a complementary improvement to MSDNets, as it enables smaller MSDNets to reach the final error rates of bigger MSDNets, while having similar or better early predictions. 



%%%%%%%%%%%%%%%%%% EANN %%%%%%%%%%%%%%%%%%%%%%%%

\subsection{EANN: Closing Early Performance Gaps by Delaying Final Predictions.}
\label{sec:eann_experiment}



\textbf{EANNs on CIFAR100.} In Fig.~\ref{fig:eann_f}, we assemble Res\anns to form EANNs\footnote{The Res\anns have $c=32$ and $n=7, 13, 25$, so that they form an EANN with an exponential base $b\approx 2$. 
By proposition~\ref{prop:eann}, the average cost inflation is $E[C]\approx 2.44$ for $b=2$, so that the EANN should 
compete against the OPT of $n=20$, using $2.44$ times of original costs.} on CIFAR100 and make three observations.
First, EANNs are better than the \ann in early computation, because the ensembles dedicate early predictions to small networks. Even though \const has the best early predictions as in Table~\ref{tab:compare_f}, it is still better to deploy small networks. 
Second, because the final prediction of each network is kept for a long period, \adaloss leads to significantly better EANNs than \const does, thanks to the superior final predictions from \adaloss. 
Finally, though EANNs delay computation of large networks, it actually appears closer to the OPT, because of accuracy saturation. Hence, EANNs should be considered when performance saturation is severe. 

\textbf{EANN on ILSVRC.}
\cite{msdense} and \cite{feedbacknet} use ensembles of networks of linearly growing sizes as baseline anytime predictors. However, in Fig.~\ref{fig:compare_ensemble}, an EANN using Res\anns of depths 26, 50 and 101 outperforms the linear ensembles of ResNets and DenseNets significantly on ILSVRC.
In particular, this drastically reduces the gap between ensembles and the state-of-the-art anytime predictor MSDNet~\cite{msdense}. 
Comparing Res\ann50 and the EANN, we note that the EANN achieves better early accuracy but delays final predictions. 
As the accuracy is not saturated by Res\ann26, the delay appears significant. Hence, EANNs may not be the best when the performance is not saturated or when the constant fraction of extra cost is critical. 


\subsection{Data-set Difficulty versus Adaptive Weights}
\label{sec:weight_vs_dataset}



In Fig.~\ref{fig:adaloss_weights}, we plot the final \adaloss weights of the same Res\ann model (25,32) on CIFAR10, CIFAR100, and SVHN to study the effects of the data-sets on the weights. We observe that from the easiest data-set, SVHN, to the hardest, CIFAR100, the weights are more concentrated on the final layers. This suggests that \adaloss can automatically decide that harder data-sets need more concentrated final weights to have near-optimal final performance, whereas on easy data-sets, more efforts are directed to early predictions. Hence, \adaloss weights may provide information for practitioners to design and choose models based on data-sets.

\section{Conclusion and Discussion}
This work devises simple adaptive weights, \adaloss, for training anytime predictions in DNNs. We provide multiple theoretical motivations for such weights, and show experimentally that adaptive weights enable small \anns to outperform large \anns with the commonly used non-adaptive constant weights. Future works on adaptive weights includes examining \adaloss for multi-task problems and investigating its ``first-order'' variants that normalize the losses by individual gradient norms to address unknown offsets of losses as well as the unknown scales. We also note that this work can be combined with orthogonal works in early-exit budgeted predictions~\cite{cascade_nn,adaptivenn} for saving average test computation. 


\section*{Acknowledgements} 
This work was conducted in part through collaborative participation in the Robotics Consortium sponsored by the U.S Army Research Laboratory under the Collaborative Technology Alliance Program, Cooperative Agreement W911NF-10-2-0016. 

%
%%%%%%%%%%%%%%%%%%% BIB %%%%%%%%%%%%%%%%%%%%%%%%
%\bibliographystyle{aaai}
%{
%\bibliography{AAAI-HuH.2508}
%}
%
%%%%%%%%%%%%%%%%%%% APPENDIX %%%%%%%%%%%%%%%%%%%%%%%%
%\appendix
%



\section{Sketch of Proof of Proposition~\ref{prop:eann}}
\begin{proof}
For each budget consumed $x$, we compute the cost $x'$ of the optimal that EANN is competitive against. The goal is then to analyze the ratio $C = \frac{x}{x'}$. 
The first ANN in EANN has depth 1. The optimal and the result of EANN are the same. Now assume EANN is on depth $z$ of ANN number $n+1$ for $n\geq 0$, which has depth $b^{n}$. \\
(Case 1) For $z \leq b^{n-1}$, EANN reuse the result from the end of ANN number $n$. 
The cost spent is $x = z + \sum _{i=0}^{n-1} b^i = z + \frac{b^n-1}{b-1}$. 
The optimal we compete has cost of the last ANN, which is $b^{n-1}$
The ratio satisfies:
\begin{align*} 
C &= x / x' = \frac{z}{b^{n-1}} + 1 + \frac{1}{b-1} - \frac{1}{b^{n-1}(b-1)} \\
&\leq 2 + \frac{1}{b-1} - \frac{1}{b^{n-1}(b-1)} 
< 2+ \frac{1}{b-1}. 
\end{align*}
Furthermore, since $C$ increases with $z$, 
\begin{align*}
&E_{z \sim Uniform(0, b^{n-1})}[C] \\
&\leq b^{1-n} \int _0 ^{b^{n-1}} 
    z b^{1-n}+ 1 + \frac{1}{b-1} \;dz \\
&= 1.5 + \frac{1}{b-1}.
\end{align*}
\\
(Case 2) For $b^{n-1} < z \leq b^n$, EANN outputs anytime results from ANN number $n+1$ at depth $z$. 
The cost is still $x = z +\frac{b^n-1}{b-1}$. The optimal competitor has cost $x' = z$.  Hence the ratio is 
\begin{align*}
C &= x/ x' = 1 + \frac{b^n-1}{z(b-1)} \\
&\leq 2 + \frac{1}{b-1} - \frac{1}{b^{n-1}(b-1)} 
< 2+ \frac{1}{b-1}.
\end{align*}
Furthermore, since $C$ decreases with $z$, 
\begin{align*}
&E_{z \sim Uniform(b^{n-1}, b^n)}[C] \\
& \leq 1 +  
   \frac{1}{b^n - b^{n-1}} \int _{b^{n-1}} ^{b^{n}} 
        \frac{b^n -1}{z(b-1)} \; dz  \\
&= 1 + \frac{(b - b^{1-n}) \ln{b}}{(b-1)^2} \\
&< 1 + \frac{b \ln{b}}{(b-1)^2} 
\end{align*}

Finally, since case 1 and case 2 happen with probability $\frac{1}{b}$ and $(1-\frac{1}{b})$, we have
\begin{align}
    \sup _B C &= 2+ \frac{1}{b-1} \\
\intertext{and}
    E_{B\sim Uniform(0, L)}[C] &\leq 1 - \frac{1}{2b} + \frac{1}{b-1} + \frac{\ln{b}}{b-1}.
\end{align}
We also note that with large $b$, $\sup _B C \rightarrow 2$ and $E[C] \rightarrow 1$ from above.
\end{proof}

If we form a sequence of regular networks that grow exponentially in depth instead of \ann, then the worst case happen right before a new prediction is produced. Hence the ratio between the consumed budget and the cost of the optimal that the current anytime prediction can compete, $C$, right before the number $n+1$ network is completed, is 
\[
    \frac{\sum _{i=1}^n b^i}{b^{n-1}} \xrightarrow[]{n\rightarrow \infty} \frac{b^2}{b-1} = 2 + (b-1) + \frac{1}{b-1} \geq 4. 
\]
Note that $(b-1) + \frac{1}{b-1} \geq 2$ and the inequality is tight at $b=2$. Hence we know $\sup _B {C}$ is at least 4. Furthermore, the expected value of $C$, assume $B$ is uniformly sampled such that the interruption happens on the $(n+1)^{th}$ network, is:
\begin{align*}
    E[C] &= \frac{1}{b^{n}} \int _0 ^{b^{n}} \frac{x + \frac{b^{n}-1}{b-1}}{b^{n-1}} \; dx \\ &\xrightarrow[]{n\rightarrow \infty} 1.5 + \frac{b-1}{2} + \frac{1}{b-1} \geq 1.5 + \sqrt{2} \approx 2.91.
\end{align*}
The inequality is tight at $b = 1 + \sqrt{2}$. With large $n$, since almost all budgets are consumed by the last few networks, we know the overall expectation $E_{B\sim Uniform(0, L)}[C]$ approaches $1.5 + \frac{b-1}{2} + \frac{1}{b-1}$, which is at least $1.5 + \sqrt{2}$.




\section{Implementation Details of \anns}
\label{sec:implementation_ann}


\textbf{CIFAR and SVHN Res\anns.} For CIFAR10, CIFAR100~\cite{cifar}, and SVHN~\cite{svhn}, Res\ann follow \cite{resnet} to have three blocks, each of which has $n$ residual units. Each of such basic residual units consists of two 3x3 convolutions, which are interleaved by BN-ReLU. A pre-activation (BN-ReLU) is applied to the input of the residual units. The result of the second 3x3 conv and the initial input are added together as the output of the unit. The auxiliary predictors each applies a BN-ReLU and a global average pooling on its input feature map, and applies a linear prediction. The auxiliary loss is the cross-entropy loss, treating the linear prediction results as logits. For each $(n,c)$ pair such that $n < 25$, we set the anytime prediction period $s$ to be 1, i.e., every residual block leads to an auxiliary prediction. We set the prediction period $s=3$ for $n=25$. 


\textbf{Res\anns on ILSVRC.} Residual blocks for ILSVRC are bottleneck blocks, which consists of a chain of 1x1 conv, 3x3 conv and 1x1 conv. These convolutions are interleaved by BN-ReLU, and pre-activation BN-ReLU is also applied. Again, the output of the unit is the sum of the input feature map and the result of the final conv. 
Res\annnp50 and 101 are augmented from ResNet50 and 101~\cite{resnet}, where we add BN-ReLU, global pooling and linear prediction to every two bottleneck residual units for ResNet50, and every three for ResNet101. 
We create Res\annnp26 for creating EANN on ILSVRC, and Res\annnp26 has four blocks, each of which has two bottleneck residual units. The prediction period is every two units, using the same linear predictors. 


\textbf{Dense\anns on ILSVRC.} We augment DenseNet169~\cite{densenet} to create Dense\ann169. 
DenseNet169 has 82 dense layers, each of which has a 1x1 conv that project concatenation of previous features to $4k$ channels, where $k$ is the growth rate~\cite{densenet}, followed by a 3x3 conv to generate $k$ channels of features for the dense layer. The two convs are interleaved by BN-ReLU, and a pre-activation BN-ReLU is used for each layer. The 82 layers are organized into four blocks of size 6, 12, 32 and 32. Between each neighboring blocks, a 1x1 conv followed by BN-ReLU-2x2-average-pooling is applied to shrink the existing feature maps by half in the hight, width, and channel dimensions. We add linear anytime predictions every 14 dense layers, starting from layer 12 (1-based indexing). The original DenseNet paper~\cite{densenet} mentioned that they use drop-out with keep rate 0.9 after each conv in CIFAR and SVHN, but we found drop-out to be detrimental to performance on ILSVRC.


\textbf{MSDNet on ILSVRC.} MSDNet38 is described in the appendix of~\cite{msdense}. We set the four blocks to have 10, 9, 10 and 9 layers, and drop the feature maps of the finest resolution after each block as suggest in the original paper. 
We successfully reproduced the published results to 24.3\% error rate on ILSVRC using our Tensorflow implementation. We used the original published results for MSDNet38+\const in the main text. We use MSDNet33, which has four blocks of 8, 8, 8 and 9 layers, for the small network that uses \adaloss. We predict using MSDNet33 every seven layers, starting at the fifth layer (1-based indexing). 



\section{Additional Details of \adaloss}

\label{sec:implementation_adaloss}


\subsection{Weight Regularization} 
In practice, some expected loss $\ell_i$ could be much larger than the other losses, so that \adaloss may assign such $\ell_i$ too small a weight for it to receive enough optimization to recover. To prevent this, we mix the uniform constant weight with \adaloss as a form of regularization as follows in Eq.~\ref{eq:geometric_arithmetic_mean}. Such mixture prevents the weight of $\ell_i$ from being too close to zero. 
\begin{align}
    \min _{\theta} \sum _{i=1}^L \big( \alpha (1 - \gamma) \ln \ell _i (\theta) + \gamma  \ell_i (\theta) \big),
    \label{eq:geometric_arithmetic_mean}
\end{align}
where $\alpha >0$ and $\gamma >0$ are hyper parameters. In practice, since DNNs often have elaborate learning rate schedules that assume $B_L=1$, we choose $\alpha  = \min_i \hat{\ell}_i(\theta)$ at each iteration to scale the max weight to 1. We choose $\gamma = 0.05$ from validation sets on CIFAR10 and CIFAR100 from the set $\{0, 0.05, 0.15\}$.

\subsection{Ablation Study of \adaloss parameters on CIFAR} 


\begin{table}[t]
    \centering
    \begin{tabular}{c|cccc|c}
\hline
$\gamma$ & 1/4 & 1/2 & 3/4 & 1  & sum\\
\hline
0.0
	&  0.00 &  0.00 &  0.00 &  0.00  & 0.00\\
0.05
	& -20.08 & -2.15 &  2.22 &  2.43 & -17.59 \\
0.15
	& -23.88 & -0.20 &  5.18 &  5.17 & -13.72\\
\hline
    \end{tabular}
    \caption{Relative percentage increase in error rate by switching from $\gamma=0$. (lower is better.) A small amount of $\gamma = 0.5$ drastically improves early predictions without increasing late error rate much.}
    \label{tab:adaloss_gamma}
\end{table}


\begin{table}[t]
    \centering
    \begin{tabular}{c|cccc}
\hline
EMA $m$ & 1/4 & 1/2 & 3/4 & 1 \\
\hline
0.9
	&  0.00 &  0.00 &  0.00 &  0.00 \\
0.99
	& -0.29 &  0.25 &  0.05 &  0.15 \\
\hline
    \end{tabular}
    \caption{Relative percentage increase in error rate by switching from $m=0.9$.  (lower is better.) The two options essentially result in the same error rates.}
    \label{tab:adaloss_momentum}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|cccc}
\hline
Update period $e$ & 1/4 & 1/2 & 3/4 & 1 \\
\hline
1
	&  0.00 &  0.00 &  0.00 &  0.00 \\
100
	&  0.71 &  0.23 &  0.24 &  0.45 \\
\hline
    \end{tabular}
    \caption{Relative percentage increase in error rate by switching from $e=0$.  (lower is better.) The options are essentially the same on CIFAR10 and CIFAR100. }
    \label{tab:adaloss_update_per}
\end{table}

We conduct ablation studies for the parameters of \adaloss: (1) $\gamma$ in Eq.~\ref{eq:geometric_arithmetic_mean}, which is the mixture weight of the uniform static weighting, (2) the exponential moving average (EMA) momentum, $m$, for updating the expected loss $\hat{\ell}_i$ at each stochastic gradient descent (SGD) step, and (3) the number of SGD steps $e$ to wait between updating \adaloss weights $B_i$ using the learned $\hat{\ell}_i$. We choose $\gamma \in \{0, 0.05, 0.15 \}$, $m \in \{0.9, 0.99\}$, and $e \in \{1, 100\}$, and evaluate them on CIFAR10 and CIFAR100 ResANNs whose $n \in \{9,17,25\}$ and $c=32$. Over the 72 experiments, we found the effects of $m$, and $e$ are almost negligible, as they generate $<0.5\%$ of relative difference in error rates on average, which translates to $0.1\%$ absolute error difference on CIFAR100. These comparisons are in Table~\ref{tab:adaloss_momentum} and Table~\ref{tab:adaloss_update_per}. In the experiment sections, we choose $m = 0.9$ and $e=1$.

However,  $\gamma$ does affect the performance significantly, as show in Table~\ref{tab:adaloss_gamma}. $\gamma=0$ means pure \adaloss and $\gamma=1$ means \const. We observe that with $\gamma=0.05$, the small amount of uniform static weight reduces the error rate at 1/4 of the total cost by 20\% relatively, but at the cost of minor 2.5\% relative increase in late predictions. Increasing $\gamma$ further to $0.15$ has only marginal benefits to early predictions, but has the same negative impact to late accuracy. This suggests that while a small $\gamma$ helps, we should only use a small amount. Throughout the experiment sections in the main text, we choose $\gamma = 0.05$.


