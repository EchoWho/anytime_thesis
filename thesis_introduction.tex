

When evaluating a predictor for an application, one often needs to consider two critical aspects of algorithms: the accuracy of the prediction, and the computational cost of the predictor. These two factors are often opposed to each other: practitioners typically have the dilemma to choose between predictors that are accurate but slow to compute and ones that are fast but inaccurate. 
This trade-off between computational cost and accuracy is inherently difficult to manage, and is the focus of this work.


\section{Motivations and Problem Settings}

Machine learning algorithms typically have computational budget limits during test-time. For applications on mobile devices and Internet of Things (IoTs), it is critical for the predictors to fit in these devices with low computational power and consume little computation during test-time. For robotic applications such as autonomous vehicle or drones, it is paramount to have low latency in visual detection for planning maneuvers. Web services such as Email spam filters also require low latency to maintain user satisfaction. 
For such applications, one often cannot deploy predictors that achieve the state-of-the-art accuracy, because those predictors often are associated with high computational costs. Instead, these applications often seek the most accurate models that are within their computational budgets. 

Furthermore, the computational budget limits of many applications can also vary during test-time, or can be agnostic during training. 
For instance, robotic applications have varying test-time budget limits based on the speed of the robot and the complexity of the environments. 
Web servers may handle heavier query traffic during the day than during the night, but they are expected to maintain the same low latency. 
Mobile and low-computation devices may want a low-power mode in case of low battery. 
Hence, it is beneficial to consider the setting where we seek the most accurate models at each possible budget limit of the applications. 
We draw special attention to the fact that under this setting, we delay the decision of the budget limits to the test-time, allowing greater flexibility in the algorithms. Furthermore, when the budget limit becomes known, one can extract from the spectrum of cost-efficient models. Alternatively, if one wants to minimize the average test-time computational costs of the prediction given a target accuracy level, such as in budgeted prediction~\ref{cascade_nn,adaptivenn}, one can combine the spectrum of models with early-stopping policies in order to reduce computation on clear decisions and prolong computation on ambiguous ones.

While the above examples and problem settings focus on the balance between model accuracy and test-time computation, practitioners may often be concerned with their train-time computational costs. 
One reason for this concern is that the data-sets are becoming larger and are continuously updated due to improved data storage and collection. Specifically, fields such as finance, information retrieval, computer vision, text and vocal language processing, and etc., each has accumulated data from decades of practice. Training state-of-the-art models against the decades worth of data becomes increasingly challenging. Hence, it is crucial for modern machine learning models to be able to handle large data-sets that may not be present on the same machine or at even the same time. Furthermore, the models ideally should be able to be improved as more data becomes available or more train-time computational resource is allocated. 

Another key reason for the rising train-time computation is the increased model complexity. As neural networks become the dominating methods for 
computer vision tasks and natural language processing, practitioners often rely on experts to find optimal network architectures via trial-and-error. However, such experimental process can be vastly expensive, taking thousands of GPU-days~\citep{nas}, and may yet generalize to general data-sets. Facing such complex and combinatorial hyper-parameter space of model architectures, we need guidance to search in a cost-efficient manner. In particular, practitioners may be interested in increasing the model complexity gradually: one starts with small architectures that can be trained and deployed easily; then as more train-time computation is allowed, the model complexity is gradually increased in a guided manner. Furthermore, ideally one would like the models to reusable and stable, so that new models can utilize previous found models and do not deviate from previous models too much to affect user experience. 

In summary, the targeted problem settings of this work can be partitioned into two parts. The first focuses on the problem of finding accurate predictions at each possible test-time computational budgets, and the second focuses on enabling the previous solutions to handle the rising computational cost in training due to increased data-set sizes and increased complexity in model hyper-parameters. 

\section{Approach}

For each of test-time and train-time trade-off between computation and accuracy, we develop both algorithms with theoretical performance guarantees and algorithms that work well on real-world data-sets. We summarize the main approaches that we take as follows. 

One approach to have accurate predictions at each possible test-time computational budget limit is to first produce crude predictions early, and then continuously improve them. Such algorithms are called \emph{anytime} algorithms, and they automatically adjust to the varying or agnostic test-time budget limits because the algorithm utilize the computational resources until the budget limit. 
One common approach to achieve anytime prediction is through ensembles of weak predictors~\citep{sochman:05,brubaker:07,lefakis:10,reyzin:11,xu:14,cai:15,speedboost}, so that at test-time, one computes the predictors iteratively and then reports the partial ensemble result as the intermediate or anytime results. Indeed, the first anytime predictor of this work follows this idea of combining weak predictors in a generalized linear prediction setting, and utilizes submodular optimization results~\citep{kemp} to bound the performance of the predictions. Furthermore, we prove a limitation of any anytime algorithm that stems from an ensemble of weak predictors, proving that in general such an ensemble of cost $B$ can only achieve comparable rewards of the optimal ensemble of cost $B/4$. 

Such limitations lead us to consider an alternative approach to anytime prediction, where we train a single model to produce multiple intermediate results for anytime predictions, and we optimize all the anytime results jointly. 
Viewing anytime prediction as a multi-objective optimization, we develop an adaptive method to balance the weights of the objectives, and improve anytime prediction quality on multiple neural networks and data-sets. 
By exploiting anytime neural networks as weak learners, we can form ensembles of anytime predictors for anytime prediction, and interestingly, by making weak learners to be anytime predictors, we can circumvent the previous theoretical limitation on ensemble-based anytime predictors.

For the train-time trade-off between computation and accuracy, we specifically target problems that are often accompanied by our anytime predictors. Since anytime models often stem from ensembles of weaker models that are trained sequentially, they can be difficult to scale to large data-sets, especially on data-sets that may be expensive to loop through. To address this weakness, we develop gradient boosting algorithms for stochastic data streams so that we can train all weak learners jointly. Gradient boosting can be considered as approximated gradient descent in the functional space, and can be analyzed via gradient descent~\citep{hazan2007logarithmic,grubb2011generalized}. Combining gradient boosting with analysis from online learning~\citep{beygelzimer2015optimal,cesa2004generalization}, we analyze the proposed algorithms under stochastic data streams. We bound the regrets of the proposed algorithm, showing that it achieves no regret in prediction losses against any competitor under strongly convex losses and under the assumption that each weak online learner can predict better than random by a margin.

We also address the increased complexity of hyper parameters in the specific setting of neural architecture search~\citep{nas,Elsken2018NeuralAS}, where one seeks the optimal architecture given a data-set and an optimization objective. We first formulate the problem as a bi-level optimization and show its connections to the earlier anytime linear prediction problem. 
Inspired by forward selection approach in the linear prediction setting, we propose to expand existing neural networks models iteratively guided by gradient boosting. 
Each step of the model expansion is determined by fitting potential short-cut connections against the gradient of the loss with respect to intermediate layers. 

In summary, the thesis statement of this work is as follows. 
\begin{quotation}
%This work meows a lot but achieves no fish. 
\textbf{Thesis Statement:} 
Modern machine learning applications often have to address the trade-off between computational costs and predictive powers. 
%At test-time, the computational budget is often limited, and it may be agnostic during training or vary during test. Furthermore, thanks to the increased amount of data storage and parallel computation, the training cost of models is increasing rapidly to thousands of GPU-days. 
This work addresses the trade-off between computational speed and prediction accuracy at both test-time and training-time, providing theoretical performance guarantees and practical experimental results. 
Specifically, for dynamically trading speed and accuracy at test-time, we leverage cost-greedy methods to achieve near-optimal anytime linear predictions, and we 
also prove anytime performance upper bound on such ensemble-based methods in general. 
However, utilizing multi-objective optimization,
we show such upper bound can be avoided via ensemble of anytime weak learners.
To address the rising problem of training computation, we propose to adapt ensemble methods to stochastic data-streams. 
Furthermore, we draw connection between anytime prediction and neural architecture search, 
and develop practical algorithms to expand network architectures iteratively in order to explore the vast space of networks efficiently during training. 
\end{quotation}


\section{Overview of Chapters and Their Contributions}

This section provides an overview of each chapter and its contributions to the development of cost-effective predictors. 
Chapter~\ref{chapter:anytime_linear} and Chapter~\ref{chapter:ann} cover the advancement in anytime predictors for cost-efficient and versatile predictors at test-time. Chapter~\ref{chapter:sgb} and Chapter~\ref{chapter:nas} each addresses a common concern regarding the training cost of previous anytime models. 

Chapter~\ref{chapter:anytime_linear} covers the anytime linear prediction under the setting where features are computed in groups and feature groups have costs. Under this setting, we learn the order to compute the features, and at test-time, we update the latest linear prediction whenever new features are computed. In Section~\ref{sec:gomp_method}, we extend feature selection methods Forward Regression (FR) and Orthogonal Matching Pursuit (OMP) to handle feature groups that have costs. We then provide theoretical analysis of these two cost-greedy algorithms in Section~\ref{sec:gomp_proof}, utilizing spectral analysis and sub-modular optimization. We first prove that both algorithms achieve near-optimal linear predictions in terms of explained variance, at test-time budgets where the algorithms just finish computing new feature groups. 
Then we show that such bounds are inadequate for bounding performance for all test-time budgets. Instead we propose a simple modification, called doubling, to the previous cost-greedy procedure in Section~\ref{sec:gomp_bi_criteria}, and provide theoretically analysis that the modified algorithms is near-optimal at any test-time budget $B$ in comparison to the optimal at budget $B/4$ in Theorem~\ref{thm:greedy.doubling-greedy-bound-approx}. We further show that the constant $B/4$ is tight in Theorem~\ref{thm:greedy.biapproximation-upper-bound}, which shows that it is impossible in general for anytime algorithms via ensembles to compare against the optimal of a budget that is more than $B/4$. The contribution of this chapter is summarized as follows.

\begin{enumerate}
\item We cast the problem of anytime linear prediction 
as a feature group sequencing problem  
and propose extensions to Forward Regression (FR) and Orthogonal Matching Pursuit (OMP)
under the setting where features are in
groups that have costs. 
\item We theoretically analyze our extensions to FR and OMP 
and show that they both achieve $(1-e^{-\lambda^*})$ near-optimal 
explained variance with linear predictions at budgets when 
they choose feature groups, where $\lambda^*$ is a constant related to how correlated the 
features groups are to each other.
\item We develop the first anytime algorithm 
with provably performance guarantees at \textit{any} budget limit $B$, 
by relating the achieved prediction performance to that of the optimal of cost 
$B/4$. 
\item We further show that the fraction $1/4$ is tight, as in that it is 
impossible to achieve multiplicative bounds of the prediction performance at $B$ 
against any optimal of cost greater than $B/4$. 
\item The previous pair of theoretical results  present a tight bound on 
anytime predictions based on ensemble of weak predictors.
\end{enumerate}

As Chapter~\ref{chapter:anytime_linear} seals the fate of anytime predictors via ensembles of weak learners, we move on in Chapter~\ref{chapter:ann} to develop anytime predictors within neural networks, which work well in practice on real-world data-sets such as ILSVRC2012~\citep{ILSVRC15}. We pose the training of anytime predictors within a single network as a multi-objective optimization, and propose to balance the weights of the anytime objectives adaptively in a weighted sum in Section~\ref{sec:ann}. The adaptive weight balancing intuitively normalizes the losses so that they have the same scale. This simple modification can be derived from three theoretical considerations, including maximum likelihood models, optimization with log-barriers, and optimization of geometric mean of the expected anytime losses. We also show experimentally in Section~\ref{sec:ann_experiment_questions} that the proposed weight balancing leads to better anytime predictions within the networks across multiple architectures and data-sets. The anytime neural networks also allow us to revisit the limitation of anytime predictors via ensembles. In fact, we show in Section~\ref{sec:eann} that an ensemble of anytime neural networks can circumvent the previous hard example, so that the performance at a test-time budget $B$ is comparable to the optimal at budget $B/C$, where the constant $C$ can be smaller than $4$. This suggests that future anytime predictors should combine weak learners that are anytime predictors on their own, instead of regular weak predictors.
The contribution of this chapter is summarized as follows.

\begin{enumerate}[resume]
\item For training anytime predictions within neural networks, we derive an adaptive weight scheme for anytime losses from multiple theoretical considerations, and show that experimentally this scheme achieves near-optimal final accuracy \emph{and} competitive anytime ones on multiple data-sets and models.
\item We assemble anytime neural networks of exponentially increasing depths to achieve near-optimal anytime predictions at every budget at the cost of a constant fraction of additional consumed budget, under the assumption that each anytime neural network is near-optimal in its later fraction of depths. We verify that this assumption holds practically in current state-of-the-art networks.
\item The near-optimal guarantee of ensemble of anytime neural networks breaks the earlier hardness result of anytime predictors from ensemble of regular predictors by increasing the constant $1/4$ to $1/2.91$.
\end{enumerate}

Starting from Chapter~\ref{chapter:sgb}, we switch the topic from test-time balance of computation and accuracy to the training-time cost-effectiveness. Chapter~\ref{chapter:sgb} covers how to train an ensemble for gradient boosting given a stochastic stream of data samples. Gradient boosting is a common way to form ensemble of weak learners, and each weak learner is trained to match the functional gradient of the loss with respect to the current predictor. We set up these preliminary details in Section~\ref{sec:sgb_preliminaries}. Such boosting can suffer on large data-sets, because it trains the weak learners sequentially and loop the data many passes. To address this weakness of gradient boosting, we propose a modification to handle stochastic data streams in Section~\ref{sec:sgb_algorithm}, so that all weak learners are online learners and are trained and optimized jointly. Combining theoretical analysis of convex optimization for gradient boosting and that of online learning for handling stochastic streams, we prove in Theorem~\ref{them:smooth_strongly_convex} and Theorem~\ref{them:non_smooth_strongly_convex} that the proposed algorithms can achieve no regret against any competitor under convex losses and under the assumptions that the weak online learners are better than random by a margin. 
The contribution of this chapter is summarized as follows.

\begin{enumerate}[resume]
\item Assuming a non-trivial edge can be achieved by each deployed weak online learner, we develop gradient boosting algorithms to handle smooth or non-smooth loss functions on stochastic data streams.
\item The theoretical analysis show that under the smooth losses, 
the proposed algorithms achieves exponential decay on the average regret with respect to the number of weak learners. 
\item Under non-smooth but strongly convex losses, we show that the proposed streaming gradient boosting can instead achieve $O(\ln N/N)$ average regret with respect to the number weak learners $N$. 
\end{enumerate}

Chapter~\ref{chapter:nas} considers on the search through the hyper-parameter space, and focuses on the problem of neural architecture search. 
Traditionally this search is done manually and requires massive computational resources for training. In Section~\ref{sec:nas_bi_level_optimization}, we draw a connection from the architecture search to learning anytime predictions with ensemble methods, showing that they both solve a bi-level optimization problem, where the outer level searches for the discrete choice of architecture or weak learners, and the inner level optimizes the parameters of architecture or the weak learners. In Section~\ref{sec:search_procedure}, we develop a greedy search procedure that adds shortcut connections to existing network architectures iteratively. The added connections are chosen by matching candidate connections to the gradient of the loss with respect to intermediate layers, similar to gradient boosting of weak learners. To estimate the gradients efficiently, we initialize a large number of potential shortcut connections and train them jointly, and we utilize feature selection to keep only the most important ones. We show experimentally in Section~\ref{sec:nas_experiment} that such greedy procedure can find cost-efficient models that are at the state-of-the-art level. 
The contribution of this chapter is summarized as follows.

\begin{enumerate}[resume]
\item We propose an approach to increase complexity of neural networks during training iteratively. We alternate between two phases. The first expands the model with potential shortcut connections and train them jointly. The second phase trim the previous potential connections using feature selection and continue training the model. 
\item The proposed approach can be applied to both improve a small repeatable pattern, called cell, and improve the macro network architecture directly, unlike most popular approaches that only focus on cells. This opens up neural architecture search to fields where no domain knowledge of the macro structure exists. 
\item On cell-search, the proposed method finds a model that achieves 2.61\% error rate on CIFAR10 using 2.9M parameters within 5 GPU-days. 
%The model achieves \% error rate on ILSVRC2012 using M parameters and M multi-adds.
\item On macro-search, the proposed method finds a model that achieves 2.83\% error rate on CIFAR10 using 2.2M parameters within 5 GPU-days. 
%The model achieves \% error rate on ILSVRC2012 using M parameters and M multi-adds.
\item The proposed approach can warm start from existing networks, leveraging previous training results. Furthermore, it directly expands models on the lower convex hull of error rate vs. flops and is hence able to naturally produce a gallery of cost-effective models which is critical for production serving needs. 
\end{enumerate}



