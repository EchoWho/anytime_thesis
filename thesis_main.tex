%% This is file `elsarticle-template-1-num.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%%
%% $Id: elsarticle-template-1-num.tex 149 2009-10-08 05:01:15Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-1-num.tex $
%%
\documentclass[review,12pt]{cmuthesis}
\usepackage{times}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[numbers,sort]{natbib}
\usepackage[backref,pageanchor=true,plainpages=false, pdfpagelabels, bookmarks,bookmarksnumbered,
%pdfborder=0 0 0,  %removes outlines around hyper links in online display
]{hyperref}

% Approximately 1" margins, more space on binding side
%\usepackage[letterpaper,twoside,vscale=.8,hscale=.75,nomarginpar]{geometry}
%for general printing (not binding)
\usepackage[letterpaper,twoside,vscale=.8,hscale=.75,nomarginpar,hmarginratio=1:1]{geometry}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% The graphicx package provides the includegraphics command.
\usepackage{graphicx}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
\usepackage{lineno}

% Set the typeface to Times Roman
\usepackage{times}

\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}
\setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}
\usepackage{epsfig}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{bbold}
\usepackage{flushend}
\usepackage{subfig}
\usepackage{array}
\usepackage{comment}
\usepackage{subfig}
\usepackage{longtable}
\usepackage{hhline}
\usepackage{helvet}
\usepackage{courier}

\usepackage[final]{pdfpages}

\usepackage{enumitem}

%\usepackage[ruled,vlined,linesnumbered,resetcount]{algorithm2e}

\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{caption}
%\usepackage{subcaption}
\usepackage{xspace}
\usepackage{color}

\usepackage{amsthm}

\usepackage{bbm}
\usepackage{multirow}



%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{lemma}[theorem]{Lemma}
%
%\theoremstyle{definition}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{example}[theorem]{Example}
%\newtheorem{xca}[theorem]{Exercise}

%\theoremstyle{remark}
%\newtheorem{remark}[theorem]{Remark}

\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\angleb}[1]{\langle #1 \rangle}

\newcommand{\at}[1]{\langle #1 \rangle}
\newcommand{\XX}{\textbf{X}}
\newcommand{\YY}{\textbf{Y}}


\makeatletter
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{%
\newenvironment{rep#1}[1]{%
 \def\rep@title{#2 \ref{##1}}%
 \begin{rep@theorem}}%
 {\end{rep@theorem}}}
\makeatother

\newcommand{\defmu}{}
\newcommand{\innerprod}[3][\defmu]{{\left\langle {#2},{#3} \right\rangle}_{#1}}
\newcommand{\norm}[2][\defmu]{{\left\lVert {#2} \right\rVert}_{#1}}
\newcommand{\functional}[3][\defmu]{{\mathcal{#2}}_{#1} [#3]}
\newcommand{\risk}[2][\defmu]{\functional[#1]{R}{#2}}

\newreptheorem{theorem}{Theorem}
\newreptheorem{definition}{Definition}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newenvironment{myproof}[1][\proofname]{\proof[#1]\mbox{}}{\endproof}
 
\newcommand{\algname}{Streaming Gradient Boosting\xspace}
\newcommand{\algshort}{SGB\xspace}
\newcommand{\algnospace}{SGB} 





\setcounter{secnumdepth}{5}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\YahooDataset}{Yahoo! Learning to Rank Challenge dataset}
\newcommand{\YahooLTR}{\textsc{Yahoo!\,LTR}}
\newcommand{\GrainDataset}{Agricultural dataset}
\newcommand{\Grain}{\textsc{Agricultural}}


\newcommand{\annnames}{Anytime Neural Networks\xspace}
\newcommand{\annname}{Anytime Neural Network\xspace}
\newcommand{\ann}{ANN\xspace}
\newcommand{\annnp}{ANN} 
\newcommand{\anns}{ANNs\xspace}
\newcommand{\aannnp}{AANN}
\newcommand{\aann}{AANN\xspace}
\newcommand{\aanns}{AANNs\xspace}
\newcommand{\adaloss}{AdaLoss\xspace}
\newcommand{\sieve}{SIEVE\xspace}
\newcommand{\explin}{EXP-LIN\xspace}
\newcommand{\const}{CONST\xspace}
\newcommand{\linear}{LINEAR\xspace}
\newcommand{\round}[1]{\lfloor #1 \rceil}

% petridish
\newcommand{\stopforward}{\textrm{sf}}
\newcommand{\stopgradient}{\textrm{sg}}
\newcommand{\petridishhard}{Isolated\xspace}
\newcommand{\petridishsoft}{Joint\xspace}
\newcommand{\Petridish}{Petridish\xspace}
\newcommand{\PetridishCP}{Petridish-CP\xspace}
\newcommand{\PetridishWS}{Petridish-WS\xspace}

\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\dd}[1]{\textcolor{blue}{#1}}


\DeclareMathOperator*{\argmax}{arg\,max} 
\DeclareMathOperator*{\argmin}{arg\,min}

\allowdisplaybreaks



\newcommand{\GOMPDIR}{1_gomp}
\newcommand{\SGBDIR}{2_sgb}
\newcommand{\ANNDIR}{3_ann}
\newcommand{\NASDIR}{4_nas}



%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}


\title{
{Cost-Effective Predictions via\\ Anytime Prediction and Learning}\\
}
\author{
Hanzhang Hu}
\date{April 26th}
\Year{2019}
\trnumber{}

\committee{
J. Andrew Bagnell \\
Martial Hebert \\
Ruslan Salakhutdinov \\
Rich Caruana}



\begin{document}


\maketitle


\chapter*{Abstract}
When choosing machine learning algorithms, one often has to balance between two opposing factors, 
the computational speed and the accuracy of predictors. 
The trade-off during testing is often difficult to balance, 
because the test-time computational budget may be agnostic
at training, and the budget may vary during testing. 
Analogously, given a novel data-set, one often lacks prior knowledge in the appropriate predictor complexity
and training computation, and furthermore, may want to interrupt or prolong the training 
based on training results. 

In this work, we address these trade-offs between computation and accuracy via \emph{anytime} prediction and 
learning, which are algorithms that can be interrupted at any time and still produce valid solutions. 
Furthermore, the quality of the results improves with the consumed computation before interruption. 
With the versatility to adjust to any budget, anytime algorithms automatically
 utilize any agnostic computational budget to the maximum extent.

To address the test-time trade-off, we study anytime predictors, whose prediction computation can be interrupted during testing. 
We start with developing provably near-optimal anytime linear predictors, and derive a theoretical performance limitation
for anytime predictors that are based on ensemble methods. Then we develop practical 
anytime predictions within individual neural networks via multi-objective optimization. 
Furthermore, leveraging these anytime predictors as weak learners, we circumvent the
performance limitation on ensemble-based anytime predictors.

For the train-time trade-off, we consider the neural architecture search problem, where
one seeks the optimal neural network structure for a data-set. We draw a parallel between this bi-level 
combinatorial optimization problem and the feature selection problem for linear prediction, 
and develop an iterative network growth algorithm that is inspired by a forward selection algorithm. 
We also consider the problem of training on large data-sets, and develop 
no-regret gradient boosting algorithms for stochastic data streams.





\chapter*{Acknowledgements} 

I would like to thank J. Andrew Bagnell, Martial Hebert, Ruslan Salakhutdinov, and Rich Caruana, for serving on my thesis committee.
I appreciate them for setting aside time for me, and giving me invaluable discussions and suggestions. 

I am happy to have been working with Drew and Martial for the past years. 
They have given me countless help, despite of their busy schedules. 
They gave me incredible freedom in exploring topics of my choice, 
and were patient with me to allow me gradually formalize and mature the research ideas. 
This experience of research and development has given me many invaluable lessons, and I am grateful to have 
Drew and Martial to guide me through them, like lighthouses in a dark ocean. 

I have been fortunate to have many collaborators over the years, in roughly chronological order,
Daniel Munoz, Alexander Grubb, Allie Del Giorno, Wen Sun, Arun Venkatraman,  Debadeepta Dey, and John Langford. 
They have taught me many things both in research and in life. Without their help, this work would not have
come to be. I especially want to thank Dey for setting up my experiments on his computational resources, 
and he has run thousands of my scripts over the two years. 

\begin{itemize}
\item Chapter~\ref{chapter:anytime_linear} is based on the Ph.D. thesis of Alexander Grubb, who made an 
excellent introduction to gradient boosting and its application to anytime prediction for me. In fact the
main contributions of this chapter are improvements of unpublished results of his thesis.
\item Chapter~\ref{chapter:ann} involves a large number of experiments. Debadeepta Dey kindly offered his 
computational resources in the middle of this project, and without his help on running thousands of
scripts, this work will never come to fruition. 
\item Chapter~\ref{chapter:sgb} came out of a paper reading session of the lab. The careful analysis of 
Wen Sun and the amazing intuition of Drew led to our initial exploration of this topic. We later also extended
previous work of Alexander Grubb on gradient boosting with non-smooth losses to enable streaming gradient 
boosting to do so as well. 
\item Chapter~\ref{chapter:nas} started as an internship project at Microsoft Research under the supervision 
of Debadeepta Dey. Initially this project came out as a bag of engineer hacks that works somehow. 
Thanks to the inputs from John Langford and Rich Caruana, we were able to steer the project into a more 
motivated direction that happens to fit into this thesis nicely.
\end{itemize}

 

\tableofcontents
\listoffigures
\listoftables

\mainmatter


%%
%% Start line numbering here if you want
%%
\linenumbers
\chapter{Introduction}
\label{chapter:thesis_introduction}
\input{thesis_introduction.tex}

\chapter{Preliminaries and Background}
\label{chapter:background}
\input{thesis_preliminaries.tex}


\chapter{Anytime Linear Prediction via Feature Group Sequencing}
\label{chapter:anytime_linear}
\input{\GOMPDIR/final.tex}

\chapter{Anytime Neural Network via Adaptive Loss Balancing}
\label{chapter:ann}
\input{\ANNDIR/AAAI-HuH.2508.tex}

\chapter{Training Gradient Boosting on Stochastic Data Streams}
\label{chapter:sgb}
\input{\SGBDIR/main.tex}

\chapter{Anytime Learning via Forward Architecture Search}
\label{chapter:nas}
\input{\NASDIR/main_conference.tex}


\chapter{Discussion and Conclusion}
\label{chapter:conclusion}
\input{thesis_conclusion.tex}



%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}          ==>>  [#]
%%   \cite[chap. 2]{key} ==>>  [#, chap. 2]
%%   \citet{key}         ==>>  Author [#]

%% References with bibTeX database:
\backmatter
\bibliographystyle{plainnat}
\bibliography{1_gomp/final.bib,2_sgb/AISTATS.bib,3_ann/AAAI-HuH.2508.bib,4_nas/network_search.bib}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}


\end{document}

%%
%% End of file `elsarticle-template-1-num.tex'.
